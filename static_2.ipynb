{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f496e28-7b9d-43bf-b07f-a06c585522b5",
   "metadata": {},
   "source": [
    "1. Explain the properties of the F-distribution. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fcbf4-46a7-4939-b169-09fbc699e16d",
   "metadata": {},
   "source": [
    "1.Definition:\n",
    "\n",
    "The F-distribution is defined as the ratio of two independent chi-squared distributed random variables divided by their respective degrees of freedom. Specifically, if ( X ) and ( Y ) are independent chi-squared random variables with ( d_1 ) and ( d_2 ) degrees of freedom, respectively, then the random variable ( F ) defined as: [ F = \\frac{X/d_1}{Y/d_2} ] follows an F-distribution with ( d_1 ) and ( d_2 ) degrees of freedom.\n",
    "\n",
    "2.Degrees of Freedom:\n",
    "\n",
    "The F-distribution is characterized by two parameters: ( d_1 ) (the degrees of freedom for the numerator) and ( d_2 ) (the degrees of freedom for the denominator). These degrees of freedom affect the shape of the distribution.\n",
    "\n",
    "3Shape: \n",
    "\n",
    "The F-distribution is right-skewed, meaning it has a longer tail on the right side. As the degrees of freedom increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
    "\n",
    "4.Range: \n",
    "\n",
    "The F-distribution takes on values from 0 to ( \\infty ). It cannot take negative values.\n",
    "\n",
    "5.Mean and Variance:\n",
    "\n",
    "The mean of the F-distribution is given by: [ \\text{Mean} = \\frac{d_1}{d_1 - 2} \\quad \\text{for } d_1 > 2 ]\n",
    "The variance of the F-distribution is given by: [ \\text{Variance} = \\frac{2(d_1^2)(d_2^2)(d_2 + 1)}{d_1(d_2 - 2)^2(d_2 - 4)} \\quad \\text{for } d_2 > 4 ]\n",
    "Mode: The mode of the F-distribution is given by: [ \\text{Mode} = \\frac{d_1 - 2}{d_1} \\quad \\text{for } d_1 > 2 ] If ( d_1 \\leq 2 ), the mode is 0.\n",
    "\n",
    "6.Applications: \n",
    "\n",
    "The F-distribution is primarily used in hypothesis testing, particularly in ANOVA to compare variances across groups, and in regression analysis to test the overall significance of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ef781-7b8d-4d1c-86e0-01171bedfef7",
   "metadata": {},
   "source": [
    "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f2244-bbed-4f80-a842-1c925dbb8a43",
   "metadata": {},
   "source": [
    "The F-distribution is primarily used in several types of statistical tests, particularly those that involve comparing variances or assessing the significance of regression models. Here are the main types of statistical tests that utilize the F-distribution and the reasons for its appropriateness:\n",
    "\n",
    "1.Analysis of Variance (ANOVA):\n",
    "\n",
    "Purpose: \n",
    "\n",
    "ANOVA is used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others.\n",
    "Use of F-distribution: In ANOVA, the F-statistic is calculated as the ratio of the variance between the group means to the variance within the groups. The F-distribution is appropriate here because it allows for the comparison of variances from different groups, assuming that the samples are drawn from normally distributed populations with equal variances (homoscedasticity).\n",
    "\n",
    "2.Regression Analysis:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "In multiple regression analysis, the F-test is used to assess the overall significance of the regression model.\n",
    "Use of F-distribution: The F-statistic in regression is calculated as the ratio of the explained variance (due to the regression model) to the unexplained variance (residuals). The F-distribution is appropriate because it helps determine whether the model explains a significant amount of variability in the response variable compared to a model with no predictors.\n",
    "\n",
    "3.Comparing Two Variances:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "The F-test can be used to compare the variances of two independent samples to determine if they are significantly different.\n",
    "Use of F-distribution: The F-statistic is calculated as the ratio of the two sample variances. The F-distribution is suitable for this test because it is derived from the ratio of two chi-squared distributions, which is the basis for variance estimation.\n",
    "\n",
    "4.Nested Models:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "In the context of model selection, the F-test can be used to compare nested models (one model is a special case of the other).\n",
    "Use of F-distribution: The F-statistic assesses whether the additional parameters in the more complex model significantly improve the fit compared to the simpler model. The F-distribution is appropriate because it provides a framework for testing the significance of the increase in explained variance.\n",
    "\n",
    "5.General Linear Models:\n",
    "\n",
    "Purpose: \n",
    "\n",
    "The F-distribution is used in the context of general linear models (GLMs) to test hypotheses about the coefficients of the model.\n",
    "Use of F-distribution: Similar to regression analysis, the F-test in GLMs evaluates whether the model as a whole is significant, allowing researchers to draw conclusions about the relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b229052-226d-4462-baac-6202c442c884",
   "metadata": {},
   "source": [
    "3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
    "populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb9074-9cd7-498d-8119-a9d4a9c1ed52",
   "metadata": {},
   "source": [
    "1.Independence:\n",
    "\n",
    "The samples drawn from the two populations must be independent of each other. This means that the selection of one sample does not influence the selection of the other sample. Independence is crucial because any dependence between the samples can lead to biased results.\n",
    "\n",
    "2.Normality:\n",
    "\n",
    "The populations from which the samples are drawn should be normally distributed. While the F-test is somewhat robust to violations of this assumption, especially with larger sample sizes, significant departures from normality can affect the accuracy of the test results. If the sample sizes are small, it is particularly important to check for normality.\n",
    "\n",
    "3.Homoscedasticity (Equal Variances):\n",
    "\n",
    "The F-test assumes that the two populations have equal variances (homoscedasticity). This is a critical assumption because the F-statistic is based on the ratio of the two sample variances. If the variances are not equal, the F-test may not be appropriate, and alternative methods (such as the Welch's F-test) should be considered.\n",
    "\n",
    "4.Random Sampling:\n",
    "\n",
    "The samples should be randomly selected from their respective populations. This ensures that the samples are representative of the populations and helps to avoid bias in the results.\n",
    "\n",
    "5.Continuous Data:\n",
    "\n",
    "The data being analyzed should be continuous. The F-test is not suitable for categorical data, as it relies on the calculation of variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ddc07-eeff-48d1-aba7-0bc82ac7ca38",
   "metadata": {},
   "source": [
    "4. What is the purpose of ANOVA, and how does it differ from a t-test? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f2d74-66d1-43a0-a4ed-27d889783724",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical method used to test the hypothesis that three or more population means are equal. The primary purposes of ANOVA include:\n",
    "\n",
    "1.Comparing Multiple Groups: ANOVA allows researchers to determine whether there are statistically significant differences among the means of three or more groups. This is particularly useful in experimental designs where multiple treatments or conditions are being tested.\n",
    "\n",
    "2.Assessing Variability: ANOVA evaluates the variability within groups compared to the variability between groups. It helps to understand whether the observed differences in sample means are greater than what would be expected due to random sampling variability.\n",
    "\n",
    "3.Controlling Type I Error: When comparing multiple groups, conducting multiple t-tests increases the risk of Type I error (incorrectly rejecting the null hypothesis). ANOVA provides a single test to assess all group means simultaneously, thus controlling the overall Type I error rate.\n",
    "\n",
    "4.Identifying Sources of Variation: ANOVA can help identify which factors (independent variables) contribute to the variation in the dependent variable, especially in factorial designs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0f8b0-e89b-451d-ad0e-8d2c4bb3d4d2",
   "metadata": {},
   "source": [
    "Differences Between ANOVA and t-test\n",
    "\n",
    "While both ANOVA and t-tests are used to compare means, they differ in several key aspects:\n",
    "\n",
    "Number of Groups:\n",
    "\n",
    "t-test:\n",
    "Typically used to compare the means of two groups (independent samples t-test) or the means of two related groups (paired samples t-test).\n",
    "ANOVA: Used to compare the means of three or more groups. It can also be extended to more complex designs, such as factorial ANOVA, which examines multiple factors simultaneously.\n",
    "\n",
    "\n",
    "Hypothesis Testing:\n",
    "\n",
    "t-test:\n",
    "Tests the null hypothesis that the means of two groups are equal.\n",
    "ANOVA: Tests the null hypothesis that all group means are equal. If the ANOVA result is significant, post-hoc tests (e.g., Tukey's HSD, Bonferroni) are often conducted to determine which specific groups differ.\n",
    "\n",
    "\n",
    "Type of Data:\n",
    "\n",
    "t-test:\n",
    "Generally used for continuous data and assumes normality and homogeneity of variances for the two groups being compared.\n",
    "ANOVA: Also used for continuous data but can handle more complex designs involving multiple factors and interactions.\n",
    "Output:\n",
    "\n",
    "t-test: \n",
    "Provides a t-statistic and a p-value to determine significance.\n",
    "ANOVA: Provides an F-statistic and a p-value. The F-statisti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a3fb2-2e08-4ed9-8018-53c418bed3dc",
   "metadata": {},
   "source": [
    "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
    "than two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884d2a4-fa01-4895-8a19-8ea3e9a97b1e",
   "metadata": {},
   "source": [
    "When to Use One-Way ANOVA\n",
    "\n",
    "\n",
    "Comparing Three or More Groups:\n",
    "\n",
    "One-way ANOVA is specifically designed for situations where you want to compare the means of three or more independent groups. If you have more than two groups to compare, one-way ANOVA is the appropriate choice.\n",
    "\n",
    "Experimental Designs:\n",
    "\n",
    "In experimental settings where you have multiple treatment conditions or groups (e.g., different dosages of a drug, various teaching methods, etc.), one-way ANOVA is used to assess whether the treatment means differ significantly.\n",
    "\n",
    "Why Use One-Way ANOVA Instead of Multiple t-tests\n",
    "\n",
    "Control of Type I Error Rate:\n",
    "\n",
    "When conducting multiple t-tests, the probability of making at least one Type I error (incorrectly rejecting the null hypothesis) increases with each additional test. For example, if you conduct three t-tests at a significance level of 0.05, the overall probability of making at least one Type I error can exceed 0.05. One-way ANOVA provides a single test to evaluate all group means simultaneously, thus controlling the overall Type I error rate.\n",
    "\n",
    "\n",
    "Efficiency:\n",
    "\n",
    "One-way ANOVA is more efficient than conducting multiple t-tests because it consolidates the analysis into a single test. This reduces the number of statistical tests performed and simplifies the interpretation of results.\n",
    "\n",
    "\n",
    "Assessment of Variance:\n",
    "\n",
    "One-way ANOVA evaluates the ratio of variance between groups to variance within groups, providing a comprehensive view of the data. It assesses whether the variability among group means is greater than the variability within the groups, which is a more robust approach than comparing pairs of means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e5a8a-4fe0-4511-993f-db599790e173",
   "metadata": {},
   "source": [
    "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
    "How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe79fa1-b7a8-43df-8720-dee8ab447795",
   "metadata": {},
   "source": [
    "In ANOVA (Analysis of Variance), variance is partitioned into two main components: between-group variance and within-group variance. This partitioning is crucial for understanding how the total variance in the data can be attributed to different sources and is fundamental to the calculation of the F-statistic.\n",
    "\n",
    "\n",
    "\n",
    "Partitioning of Variance\n",
    "\n",
    "\n",
    "Total Variance:\n",
    "\n",
    "The total variance in the data is the overall variability of the observations around the grand mean (the mean of all observations combined). It is calculated as: [ \\text{Total Variance} = \\sum_{i=1}^{n} (X_i - \\bar{X}{\\text{grand}})^2 ] where ( X_i ) represents each individual observation, and ( \\bar{X}{\\text{grand}} ) is the grand mean.\n",
    "\n",
    "\n",
    "Between-Group Variance:\n",
    "\n",
    "Between-group variance measures the variability of the group means around the grand mean. It reflects how much the means of different groups differ from each other. It is calculated as: [ \\text{Between-Group Variance} = \\sum_{j=1}^{k} n_j (\\bar{X}j - \\bar{X}{\\text{grand}})^2 ] where ( k ) is the number of groups, ( n_j ) is the number of observations in group ( j ), and ( \\bar{X}_j ) is the mean of group ( j ). This component captures the effect of the independent variable (the factor being tested).\n",
    "\n",
    "\n",
    "Within-Group Variance:\n",
    "\n",
    "Within-group variance measures the variability of observations within each group around their respective group means. It reflects how much individual observations differ from their group mean. It is calculated as: [ \\text{Within-Group Variance} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (X_{ij} - \\bar{X}j)^2 ] where ( X{ij} ) represents each observation in group ( j ), and ( \\bar{X}_j ) is the mean of group ( j ). This component captures the random error or variability that is not explained by the group differences.\n",
    "Contribution to the F-statistic\n",
    "The F-statistic is calculated as the ratio of the between-group variance to the within-group variance. This ratio helps determine whether the observed differences between group means are statistically significant. The formula for the F-statistic is:\n",
    "\n",
    "[ F = \\frac{\\text{Mean Square Between}}{\\text{Mean Square Within}} = \\frac{\\text{Between-Group Variance}}{\\text{Within-Group Variance}} ]\n",
    "\n",
    "Where:\n",
    "\n",
    "Mean Square Between (MSB) is calculated as:\n",
    "[ MSB = \\frac{\\text{Between-Group Variance}}{k - 1} ] Here, ( k - 1 ) represents the degrees of freedom associated with the between-group variance.\n",
    "\n",
    "Mean Square Within (MSW) is calculated as:\n",
    "[ MSW = \\frac{\\text{Within-Group Variance}}{N - k} ] Here, ( N - k ) represents the degrees of freedom associated with the within-group variance, where ( N ) is the total number of observations.\n",
    "\n",
    "Interpretation of the F-statistic\n",
    "A large F-statistic indicates that the between-group variance is significantly greater than the within-group variance, suggesting that at least one group mean is different from the others. This would lead to the rejection of the null hypothesis (which states that all group means are equal).\n",
    "Conversely, a small F-statistic suggests that the group means are similar, and any observed differences could be attributed to random sampling variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ff6d8-b3ad-4a57-8e18-4abff246c2a2",
   "metadata": {},
   "source": [
    "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
    "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fbfb1-d141-4f27-8bc6-19f5817d9810",
   "metadata": {},
   "source": [
    "The classical (frequentist) approach to ANOVA and the Bayesian approach represent two distinct paradigms in statistical inference, each with its own methodologies for handling uncertainty, parameter estimation, and hypothesis testing. Here are the key differences between the two approaches:\n",
    "\n",
    "1. Handling Uncertainty\n",
    "\n",
    "Frequentist Approach\n",
    "\n",
    "\n",
    "Probability Interpretation: In the frequentist framework, probability is interpreted as the long-run frequency of events. For example, a p-value represents the probability of observing the data (or something more extreme) given that the null hypothesis is true.\n",
    "Confidence Intervals: Uncertainty is quantified using confidence intervals, which provide a range of values that, with a certain level of confidence (e.g., 95%), are believed to contain the true parameter. However, this does not provide a direct probability statement about the parameter itself.\n",
    "Bayesian Approach:\n",
    "\n",
    "2.Probability Interpretation: \n",
    "\n",
    "In the Bayesian framework, probability is interpreted as a degree of belief or certainty about an event. This allows for direct probability statements about parameters. For example, one can say there is a 95% probability that a parameter lies within a certain interval.\n",
    "Credible Intervals: Bayesian methods provide credible intervals, which are intervals within which the parameter is believed to lie with a certain probability, based on the posterior distribution.\n",
    "\n",
    "\n",
    "2. Parameter Estimation\n",
    "\n",
    "\n",
    "1.Frequentist Approach:\n",
    "\n",
    "Point Estimates: Parameters are typically estimated using point estimates, such as the sample mean or variance. These estimates are derived from the data without incorporating prior beliefs or information.\n",
    "Maximum Likelihood Estimation (MLE): Frequentist methods often use MLE to estimate parameters, focusing on finding the values that maximize the likelihood of observing the given data.\n",
    "Bayesian Approach:\n",
    "\n",
    "Posterior Distribution: Bayesian methods provide a full posterior distribution for parameters, which combines prior beliefs (prior distribution) with the likelihood of the observed data. This allows for a richer understanding of parameter uncertainty.\n",
    "Prior Information: The Bayesian approach allows for the incorporation of prior information or beliefs about parameters, which can be updated with new data to produce the posterior distribution.\n",
    "3. Hypothesis Testing\n",
    "\n",
    "\n",
    "Frequentist Approach:\n",
    "\n",
    "Null Hypothesis Significance Testing (NHST): The frequentist approach typically uses NHST, where a null hypothesis (e.g., all group means are equal) is tested against an alternative hypothesis. A p-value is calculated to determine whether to reject the null hypothesis.\n",
    "Fixed Significance Level: Decisions are made based on a fixed significance level (e.g., Î± = 0.05), leading to binary conclusions (reject or fail to reject the null hypothesis).\n",
    "Bayesian Approach:\n",
    "\n",
    "Bayes Factors: \n",
    "\n",
    "In Bayesian hypothesis testing, Bayes factors are used to compare the evidence for competing hypotheses. A Bayes factor quantifies how much more likely the data are under one hypothesis compared to another.\n",
    "Continuous Evidence: Bayesian methods provide a more nuanced view of evidence, allowing for continuous assessment rather than a binary decision. This means that one can express how much more likely one hypothesis is compared to another based on the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ecb89-1295-4b2d-b5b0-c1b23173cbbb",
   "metadata": {},
   "source": [
    "8. Question: You have two sets of data representing the incomes of two different professions1\n",
    "\n",
    "V Profession A: [48, 52, 55, 60, 62]\n",
    "\n",
    "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "\n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbad94fb-afe2-499b-a037-dff3d309e42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Profession A: 32.8\n",
      "Variance of Profession B: 15.7\n",
      "F-statistic: 2.089171974522293\n",
      "p-value: 0.24652429950266952\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data for the two professions\n",
    "profession_a = np.array([48, 52, 55, 60, 62])\n",
    "profession_b = np.array([45, 50, 55, 52, 47])\n",
    "\n",
    "# Calculate the variances\n",
    "var_a = np.var(profession_a, ddof=1)  # Sample variance\n",
    "var_b = np.var(profession_b, ddof=1)  # Sample variance\n",
    "\n",
    "# Calculate the F-statistic\n",
    "f_statistic = var_a / var_b\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "df_a = len(profession_a) - 1  # Degrees of freedom for profession A\n",
    "df_b = len(profession_b) - 1  # Degrees of freedom for profession B\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = 1 - stats.f.cdf(f_statistic, df_a, df_b)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Variance of Profession A: {var_a}\")\n",
    "print(f\"Variance of Profession B: {var_b}\")\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286ba64-b11b-4586-bac2-d14ec1998b91",
   "metadata": {},
   "source": [
    "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
    "average heights between three different regions with the following data1\n",
    "V Region A: [160, 162, 165, 158, 164'\n",
    "V Region B: [172, 175, 170, 168, 174'\n",
    "V Region C: [180, 182, 179, 185, 183'\n",
    "V Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a912ba7-37ae-4739-aed2-9b2d13f6081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87330316742101\n",
      "p-value: 2.8706641879370266e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data for the three regions\n",
    "region_a = np.array([160, 162, 165, 158, 164])\n",
    "region_b = np.array([172, 175, 170, 168, 174])\n",
    "region_c = np.array([180, 182, 179, 185, 183])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
    "\n",
    "# Output the results\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fd120-acea-4b17-adcb-e451cf66e99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
